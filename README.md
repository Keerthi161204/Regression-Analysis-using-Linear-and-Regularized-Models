# Experiment 3 â€“ Regression Analysis using Linear and Regularized Models

This repository contains **Experiment 3** from the *Machine Learning Algorithms Laboratory*.  
The experiment focuses on implementing **Linear Regression**, **Ridge**, **Lasso**, and **Elastic Net** models to predict **loan amount sanctioned**, analyze their performance, and study the **biasâ€“variance tradeoff**.

---

## ğŸ“Œ Experiment Details

- **Institution:** Sri Sivasubramaniya Nadar College of Engineering, Chennai  
- **Affiliation:** Anna University  
- **Degree & Branch:** B.E. Computer Science & Engineering  
- **Semester:** VI  
- **Subject Code & Name:** UCS2612 â€“ Machine Learning Algorithms Laboratory  
- **Academic Year:** 2025â€“2026 (Even Semester)  
- **Batch:** 2023â€“2027  

---

## ğŸ¯ Objective

To implement and compare:
- Linear Regression  
- Ridge Regression  
- Lasso Regression  
- Elastic Net Regression  

for **loan amount prediction**, evaluate them using regression metrics, visualize predictions and residuals, and analyze **overfitting, underfitting, bias, and variance**.

---

## ğŸ“‚ Dataset

- **Loan Amount Prediction Dataset** (Kaggle)  
- **Target Variable:** Loan Amount Request (USD)  

ğŸ”— Dataset link:  
https://www.kaggle.com/datasets/phileinsophos/predict-loan-amount-data

---

## ğŸ§° Libraries Used

- **Pandas** â€“ Data loading and preprocessing  
- **NumPy** â€“ Numerical computation  
- **Matplotlib** â€“ Plotting and visualization  
- **Seaborn** â€“ Statistical visualization  
- **Scikit-learn** â€“ Model building, preprocessing, evaluation  

---

## ğŸ¤– Regression Models Used

- Linear Regression  
- Ridge Regression (L2 regularization)  
- Lasso Regression (L1 regularization)  
- Elastic Net Regression (L1 + L2 regularization)  

---

## ğŸ§ª Experiment Workflow

### 1ï¸âƒ£ Data Loading and Exploration
- Load training and testing datasets
- Inspect dataset using `.head()`, `.info()`, `.describe()`
- Visualize feature distributions using box plots

### 2ï¸âƒ£ Data Preprocessing
- Handle missing values using median imputation
- One-hot encode categorical variables
- Align training and test datasets
- Separate features and target variable

### 3ï¸âƒ£ Trainâ€“Validation Split
- Split data into training and validation sets (80:20)

### 4ï¸âƒ£ Feature Scaling
- Standardize numerical features using `StandardScaler`

### 5ï¸âƒ£ Model Training
- Train Linear Regression model
- Tune Ridge, Lasso, and Elastic Net using `GridSearchCV`

### 6ï¸âƒ£ Model Evaluation
- Mean Absolute Error (MAE)
- Mean Squared Error (MSE)
- Root Mean Squared Error (RMSE)
- RÂ² Score

### 7ï¸âƒ£ Visualization
- Target variable distribution
- Predicted vs actual values
- Residual plots
- Model comparison bar chart

---

## ğŸ“Š Performance Metrics

- **MAE** â€“ Mean Absolute Error  
- **MSE** â€“ Mean Squared Error  
- **RMSE** â€“ Root Mean Squared Error  
- **RÂ² Score** â€“ Coefficient of Determination  

---

## âš™ï¸ Hyperparameter Tuning Results

| Model | Best Parameters | Best CV RÂ² |
|-----|-----------------|------------|
| Ridge | Î± = 10 | 0.89 |
| Lasso | Î± = 0.01 | 0.87 |
| Elastic Net | Î± = 1, l1_ratio = 0.5 | 0.90 |

---

## ğŸ“ˆ Cross-Validation Performance

| Model | MAE | MSE | RMSE | RÂ² |
|-----|-----|-----|------|----|
| Linear | 24500 | 9.2e8 | 30331 | 0.86 |
| Ridge | 23100 | 8.5e8 | 29155 | 0.88 |
| Lasso | 23800 | 8.9e8 | 29832 | 0.87 |
| Elastic Net | 22000 | 8.1e8 | 28460 | 0.90 |

---

## ğŸ§ª Test Set Performance

| Model | MAE | MSE | RMSE | RÂ² |
|-----|-----|-----|------|----|
| Linear | 25210 | 9.6e8 | 3
